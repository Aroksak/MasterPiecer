{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "rating.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Q6t5JljWo39",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "outputId": "a8c97fd5-9135-4672-94f6-24f800224447"
   },
   "source": [
    "!pip3 install torch transformers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X6qc5vzLX2Ae",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "5a4b70c1-9866-4a3d-b660-6c1341c17371"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2uU12c7XWmA6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "PATH_TO_DATA = Path(\"drive/My Drive/Masterpiecer/data\")\n",
    "PATH_TO_MODELS = Path(\"drive/My Drive/Masterpiecer/models\")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DDd9YUq2WmBF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9VEIf1tfWmBP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    str(PATH_TO_MODELS / \"rubert_cased_L-12_H-768_A-12_pt\"),\n",
    "    do_lower_case=False\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JOY2EWdHWmBY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "45e25f65-72bc-440b-aed8-de85b03dfece"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.get_device_name(device)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Tesla P4'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3UL0Gmu-WmBi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PATH_TO_MODELS / \"rubert_cased_L-12_H-768_A-12_pt\",\n",
    "    num_labels=1,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ").to(device)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "t-raRoHmWmBs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data = pd.read_csv(PATH_TO_DATA / \"kinopoisk.csv\")\n",
    "X_raw = data['synopsis'].str.replace('\\n', ' ').values"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0c_26wmfWmB1",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "outputId": "f9ff3bf8-6202-44d1-a53e-58af4c8009b7"
   },
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in X_raw:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=320,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(data['kinopoisk_score'].values)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(train_size, \" training samples\")\n",
    "print(val_size, \" validation samples\")"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "4307  training samples\n",
      "479  validation samples\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yLhXCDNMWmB6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XshMzxNuWmB_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr=2e-5, eps=1e-8)\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0, num_training_steps=total_steps)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cCbrzGWNWmCE",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "d2a2a796-f67b-4c49-e3bc-6de24b81ce45"
   },
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(torch.float32).to(device)\n",
    "        b_labels = batch[2].to(torch.float32).to(device)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss, logits = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_attention_mask,\n",
    "                             labels=b_labels)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.float().backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(torch.float32).to(device)\n",
    "        b_labels = batch[2].to(torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            loss, logits = model(b_input_ids,\n",
    "                                 token_type_ids=None,\n",
    "                                 attention_mask=b_attention_mask,\n",
    "                                 labels=b_labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,077.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,077.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  1,077.    Elapsed: 0:00:57.\n",
      "  Batch   160  of  1,077.    Elapsed: 0:01:15.\n",
      "  Batch   200  of  1,077.    Elapsed: 0:01:34.\n",
      "  Batch   240  of  1,077.    Elapsed: 0:01:53.\n",
      "  Batch   280  of  1,077.    Elapsed: 0:02:12.\n",
      "  Batch   320  of  1,077.    Elapsed: 0:02:31.\n",
      "  Batch   360  of  1,077.    Elapsed: 0:02:50.\n",
      "  Batch   400  of  1,077.    Elapsed: 0:03:10.\n",
      "  Batch   440  of  1,077.    Elapsed: 0:03:29.\n",
      "  Batch   480  of  1,077.    Elapsed: 0:03:48.\n",
      "  Batch   520  of  1,077.    Elapsed: 0:04:07.\n",
      "  Batch   560  of  1,077.    Elapsed: 0:04:26.\n",
      "  Batch   600  of  1,077.    Elapsed: 0:04:45.\n",
      "  Batch   640  of  1,077.    Elapsed: 0:05:04.\n",
      "  Batch   680  of  1,077.    Elapsed: 0:05:23.\n",
      "  Batch   720  of  1,077.    Elapsed: 0:05:42.\n",
      "  Batch   760  of  1,077.    Elapsed: 0:06:01.\n",
      "  Batch   800  of  1,077.    Elapsed: 0:06:20.\n",
      "  Batch   840  of  1,077.    Elapsed: 0:06:39.\n",
      "  Batch   880  of  1,077.    Elapsed: 0:06:58.\n",
      "  Batch   920  of  1,077.    Elapsed: 0:07:18.\n",
      "  Batch   960  of  1,077.    Elapsed: 0:07:37.\n",
      "  Batch 1,000  of  1,077.    Elapsed: 0:07:56.\n",
      "  Batch 1,040  of  1,077.    Elapsed: 0:08:15.\n",
      "\n",
      "  Average training loss: 1.39\n",
      "  Training epoch took: 0:08:32\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.73\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,077.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,077.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  1,077.    Elapsed: 0:00:57.\n",
      "  Batch   160  of  1,077.    Elapsed: 0:01:16.\n",
      "  Batch   200  of  1,077.    Elapsed: 0:01:35.\n",
      "  Batch   240  of  1,077.    Elapsed: 0:01:54.\n",
      "  Batch   280  of  1,077.    Elapsed: 0:02:13.\n",
      "  Batch   320  of  1,077.    Elapsed: 0:02:33.\n",
      "  Batch   360  of  1,077.    Elapsed: 0:02:52.\n",
      "  Batch   400  of  1,077.    Elapsed: 0:03:11.\n",
      "  Batch   440  of  1,077.    Elapsed: 0:03:30.\n",
      "  Batch   480  of  1,077.    Elapsed: 0:03:49.\n",
      "  Batch   520  of  1,077.    Elapsed: 0:04:08.\n",
      "  Batch   560  of  1,077.    Elapsed: 0:04:27.\n",
      "  Batch   600  of  1,077.    Elapsed: 0:04:46.\n",
      "  Batch   640  of  1,077.    Elapsed: 0:05:05.\n",
      "  Batch   680  of  1,077.    Elapsed: 0:05:24.\n",
      "  Batch   720  of  1,077.    Elapsed: 0:05:43.\n",
      "  Batch   760  of  1,077.    Elapsed: 0:06:02.\n",
      "  Batch   800  of  1,077.    Elapsed: 0:06:22.\n",
      "  Batch   840  of  1,077.    Elapsed: 0:06:41.\n",
      "  Batch   880  of  1,077.    Elapsed: 0:07:00.\n",
      "  Batch   920  of  1,077.    Elapsed: 0:07:19.\n",
      "  Batch   960  of  1,077.    Elapsed: 0:07:38.\n",
      "  Batch 1,000  of  1,077.    Elapsed: 0:07:57.\n",
      "  Batch 1,040  of  1,077.    Elapsed: 0:08:16.\n",
      "\n",
      "  Average training loss: 0.64\n",
      "  Training epoch took: 0:08:34\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.74\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,077.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,077.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  1,077.    Elapsed: 0:00:57.\n",
      "  Batch   160  of  1,077.    Elapsed: 0:01:16.\n",
      "  Batch   200  of  1,077.    Elapsed: 0:01:35.\n",
      "  Batch   240  of  1,077.    Elapsed: 0:01:54.\n",
      "  Batch   280  of  1,077.    Elapsed: 0:02:14.\n",
      "  Batch   320  of  1,077.    Elapsed: 0:02:33.\n",
      "  Batch   360  of  1,077.    Elapsed: 0:02:52.\n",
      "  Batch   400  of  1,077.    Elapsed: 0:03:11.\n",
      "  Batch   440  of  1,077.    Elapsed: 0:03:30.\n",
      "  Batch   480  of  1,077.    Elapsed: 0:03:49.\n",
      "  Batch   520  of  1,077.    Elapsed: 0:04:08.\n",
      "  Batch   560  of  1,077.    Elapsed: 0:04:27.\n",
      "  Batch   600  of  1,077.    Elapsed: 0:04:46.\n",
      "  Batch   640  of  1,077.    Elapsed: 0:05:05.\n",
      "  Batch   680  of  1,077.    Elapsed: 0:05:25.\n",
      "  Batch   720  of  1,077.    Elapsed: 0:05:44.\n",
      "  Batch   760  of  1,077.    Elapsed: 0:06:03.\n",
      "  Batch   800  of  1,077.    Elapsed: 0:06:22.\n",
      "  Batch   840  of  1,077.    Elapsed: 0:06:41.\n",
      "  Batch   880  of  1,077.    Elapsed: 0:07:00.\n",
      "  Batch   920  of  1,077.    Elapsed: 0:07:19.\n",
      "  Batch   960  of  1,077.    Elapsed: 0:07:38.\n",
      "  Batch 1,000  of  1,077.    Elapsed: 0:07:57.\n",
      "  Batch 1,040  of  1,077.    Elapsed: 0:08:16.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 0:08:34\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.81\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  1,077.    Elapsed: 0:00:19.\n",
      "  Batch    80  of  1,077.    Elapsed: 0:00:38.\n",
      "  Batch   120  of  1,077.    Elapsed: 0:00:57.\n",
      "  Batch   160  of  1,077.    Elapsed: 0:01:16.\n",
      "  Batch   200  of  1,077.    Elapsed: 0:01:36.\n",
      "  Batch   240  of  1,077.    Elapsed: 0:01:55.\n",
      "  Batch   280  of  1,077.    Elapsed: 0:02:14.\n",
      "  Batch   320  of  1,077.    Elapsed: 0:02:33.\n",
      "  Batch   360  of  1,077.    Elapsed: 0:02:52.\n",
      "  Batch   400  of  1,077.    Elapsed: 0:03:11.\n",
      "  Batch   440  of  1,077.    Elapsed: 0:03:30.\n",
      "  Batch   480  of  1,077.    Elapsed: 0:03:49.\n",
      "  Batch   520  of  1,077.    Elapsed: 0:04:08.\n",
      "  Batch   560  of  1,077.    Elapsed: 0:04:27.\n",
      "  Batch   600  of  1,077.    Elapsed: 0:04:47.\n",
      "  Batch   640  of  1,077.    Elapsed: 0:05:06.\n",
      "  Batch   680  of  1,077.    Elapsed: 0:05:25.\n",
      "  Batch   720  of  1,077.    Elapsed: 0:05:44.\n",
      "  Batch   760  of  1,077.    Elapsed: 0:06:03.\n",
      "  Batch   800  of  1,077.    Elapsed: 0:06:22.\n",
      "  Batch   840  of  1,077.    Elapsed: 0:06:41.\n",
      "  Batch   880  of  1,077.    Elapsed: 0:07:00.\n",
      "  Batch   920  of  1,077.    Elapsed: 0:07:19.\n",
      "  Batch   960  of  1,077.    Elapsed: 0:07:38.\n",
      "  Batch 1,000  of  1,077.    Elapsed: 0:07:58.\n",
      "  Batch 1,040  of  1,077.    Elapsed: 0:08:17.\n",
      "\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:08:34\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.78\n",
      "  Validation took: 0:00:11\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:35:00 (h:mm:ss)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LeCZA9euaEvQ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "torch.save(model, str(PATH_TO_MODELS / \"rater_trained.dump\"))\n"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}