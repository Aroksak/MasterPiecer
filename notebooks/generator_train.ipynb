{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from yt_encoder import YTEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_TO_DATA = Path(\"../data\")\n",
    "PATH_TO_MODELS = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = YTEncoder.from_pretrained(str(PATH_TO_MODELS / \"yt.model\"))\n",
    "model = GPT2LMHeadModel.from_pretrained(str(PATH_TO_MODELS / \"s_gpt_2/\")).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    line = line.replace(\"І\", \"I\")       # yes, they differ\n",
    "    line = line.replace(\"Q\", \"q\")\n",
    "    line = line.replace(\"Y\", \"y\")\n",
    "    line = line.replace(\"Z\", \"z\")\n",
    "    line = line.replace(\"Ъ\", \"ъ\")\n",
    "    line = line.replace(\"&\", \"и\")\n",
    "    line = line.replace(\"$\", \"s\")\n",
    "    line = line.replace(\"\\xa0\", \" \")\n",
    "    line = line.replace(\"\\x97\", \" \")\n",
    "    line = line.replace(\"\\u200b\", \" \")\n",
    "    line = line.replace(\"―\", \"-\")\n",
    "    line = line.replace(\"`\", \"\")\n",
    "    line = re.sub(r\"\\s+\", \" \", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "wronged = 0\n",
    "with open(PATH_TO_DATA / \"gpt2_dataset.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = preprocess(line)\n",
    "        if 1 in tokenizer.encode(line):\n",
    "            wronged += 1\n",
    "print(wronged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_some_text(input_str, text_len = 250):\n",
    "\n",
    "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(text_len):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
    "            if next_token_id == 3:\n",
    "                break\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode([output_list])\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SynopsisDataset(Dataset):\n",
    "    def __init__(self, dataset_path=(PATH_TO_DATA / \"gpt2_dataset.txt\")):\n",
    "        super().__init__()\n",
    "        self.examples = []\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                line = \"СИНОПСИС: \" + preprocess(line)\n",
    "                self.examples.append(tokenizer.encode(line) + [3])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = SynopsisDataset()\n",
    "sampler = RandomSampler(dataset)\n",
    "synopsis_loader = DataLoader(dataset, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 3.8: 100%|██████████| 4786/4786 [13:59<00:00,  5.70it/s]\n",
      "Epoch 1, loss: 3.3: 100%|██████████| 4786/4786 [13:28<00:00,  5.92it/s]\n",
      "Epoch 2, loss: 3.1: 100%|██████████| 4786/4786 [13:27<00:00,  5.93it/s]\n",
      "Epoch 3, loss: 3.0: 100%|██████████| 4786/4786 [13:24<00:00,  5.95it/s]\n",
      "Epoch 4, loss: 2.8: 100%|██████████| 4786/4786 [13:36<00:00,  5.86it/s]\n",
      "Epoch 5, loss: 2.7: 100%|██████████| 4786/4786 [14:07<00:00,  5.65it/s]\n",
      "Epoch 6, loss: 2.6: 100%|██████████| 4786/4786 [13:33<00:00,  5.89it/s]\n",
      "Epoch 7, loss: 2.5: 100%|██████████| 4786/4786 [13:23<00:00,  5.95it/s]\n",
      "Epoch 8, loss: 2.5: 100%|██████████| 4786/4786 [13:48<00:00,  5.78it/s]\n",
      "Epoch 9, loss: 2.4: 100%|██████████| 4786/4786 [14:06<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Epoch 0==========\n",
      "\n",
      "СИНОПСИС: В фильме рассказывается о двух друзьях, которые решили вместе совершить путешествие по Южной Корее. Они решили совершить это путешествие вместе с братом по имени Оуэн, чтобы помочь ему найти путь домой, а также сделать его более безопасным от всех опасностей и опасностей.\n",
      "\n",
      "==========Epoch 1==========\n",
      "\n",
      "СИНОПСИС: Главный персонаж — агент ФБР, который пытается раскрыть тайну своего прошлого, которое он потерял при загадочных обстоятельствах, совершив головокружительную карьеру.\n",
      "\n",
      "==========Epoch 2==========\n",
      "\n",
      "СИНОПСИС: История жизни и карьеры легендарного актера Чарли Чаплина, создав неповторимое представление о жизни и кино: от первых ролей в фильмах до трагической гибели великого актера\n",
      "\n",
      "==========Epoch 3==========\n",
      "\n",
      "СИНОПСИС: В центре событий бывший агент спецслужб, ведущий расследование серии убийств. В результате своей операции он узнает страшную тайну, которая навсегда изменит его жизнь. В надежде на то, что сможет раскрыть ее и раскрыть весь мир, он отправляется в Африку. Там он знакомится с местным наркокартелем по кличке Лаки и пытается найти разгадку того, что произошло в доме его бывшего друга.\n",
      "\n",
      "==========Epoch 4==========\n",
      "\n",
      "СИНОПСИС: После того, как в один прекрасный прекрасный день героиня попадает на борт космического лайнера, она встречает своего возлюбленного в открытом космосе и влюбляется в него по уши. Однако вскоре становится ясно, что в любви не только главное чувство.\n",
      "\n",
      "==========Epoch 5==========\n",
      "\n",
      "СИНОПСИС: В центре сюжета история трех молодых женщин, которые вместе с двумя мужьями решают провести выходные с семьей в живописном районе и отметить это событие знакомством со своим лучшим другом по имени Макс. Девушки решают провести выходные с друзьями и провести это весело и открыто, чтобы не только скрасить скучную жизнь своих мужей, но и вернуть утраченное чувство юмора и уверенность в себе.\n",
      "\n",
      "==========Epoch 6==========\n",
      "\n",
      "СИНОПСИС: В центре сюжета история двух братьев, которые случайно оказываются во враждебном американском районе с единственной целью — отомстить обидчику — за убийство своих родителей.\n",
      "\n",
      "==========Epoch 7==========\n",
      "\n",
      "СИНОПСИС: В центре сюжета история семьи и их сына по имени Джонни. В детстве у них был тяжёлый случай, который лишил жизни их единственного сына\n",
      "\n",
      "==========Epoch 8==========\n",
      "\n",
      "СИНОПСИС: Фильм повествует не только о жизни и деятельности самого известного и богатого человека страны Карла Франкенштейна, но и о его любовных похождениях, любовных интригах и личных трагедиях.\n",
      "\n",
      "==========Epoch 9==========\n",
      "\n",
      "СИНОПСИС: В центре сюжета история чернокожего студента Джоуи, который оказывается в психиатрической лечебнице вместе с группой наркоманов. При этом Джоуи не осознает того факта, что все его поступки продиктованы лишь жаждой мести.\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
    "                                            num_training_steps=EPOCHS*len(dataset))\n",
    "\n",
    "proc_seq_count = 0\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    sum_loss = []\n",
    "\n",
    "    model.train()\n",
    "    t = tqdm(enumerate(synopsis_loader), total=len(dataset), desc=f\"Epoch {epoch}, loss: \")\n",
    "    for idx, batch in t:\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        sum_loss.append(loss.item())\n",
    "\n",
    "        del outputs, loss, batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        t.set_description(f\"Epoch {epoch}, loss: {np.mean(sum_loss) :.2}\")\n",
    "\n",
    "    torch.save(model.state_dict(), str(PATH_TO_MODELS / f\"gpt2_epoch_{epoch}_autostart.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    print(\"\\n\" + \"=\"*10+ f\"Epoch {epoch}\" + \"=\"*10 + \"\\n\")\n",
    "    print(generate_some_text(\" СИНОПСИС: \"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}