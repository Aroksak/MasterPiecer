{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from yt_encoder import YTEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_TO_DATA = Path(\"../data\")\n",
    "PATH_TO_MODELS = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tokenizer = YTEncoder.from_pretrained(str(PATH_TO_MODELS / \"yt.model\"))\n",
    "model = GPT2LMHeadModel.from_pretrained(str(PATH_TO_MODELS / \"s_gpt_2/\")).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    line = line.replace(\"І\", \"I\")       # yes, they differ\n",
    "    line = line.replace(\"Q\", \"q\")\n",
    "    line = line.replace(\"Y\", \"y\")\n",
    "    line = line.replace(\"Z\", \"z\")\n",
    "    line = line.replace(\"Ъ\", \"ъ\")\n",
    "    line = line.replace(\"&\", \"и\")\n",
    "    line = line.replace(\"$\", \"s\")\n",
    "    line = line.replace(\"\\xa0\", \" \")\n",
    "    line = line.replace(\"\\x97\", \" \")\n",
    "    line = line.replace(\"\\u200b\", \" \")\n",
    "    line = line.replace(\"―\", \"-\")\n",
    "    line = line.replace(\"`\", \"\")\n",
    "    line = re.sub(r\"\\s+\", \" \", line)\n",
    "    return line"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "wronged = 0\n",
    "with open(PATH_TO_DATA / \"gpt2_dataset.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = preprocess(line)\n",
    "        if 1 in tokenizer.encode(line):\n",
    "            wronged += 1\n",
    "print(wronged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_some_text(input_str, text_len = 250):\n",
    "\n",
    "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(text_len):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode([output_list])\n",
    "        print(output_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class SynopsisDataset(Dataset):\n",
    "    def __init__(self, dataset_path=(PATH_TO_DATA / \"gpt2_dataset.txt\")):\n",
    "        super().__init__()\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            self.examples = list(map(tokenizer.encode, map(preprocess, f.readlines())))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataset = SynopsisDataset()\n",
    "sampler = RandomSampler(dataset)\n",
    "synopsis_loader = DataLoader(dataset, sampler=sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def torch_memory():\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()}, cached: {torch.cuda.memory_cached()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 4.1: 100%|██████████| 4786/4786 [13:20<00:00,  5.98it/s]\n",
      "loss: 3.9: 100%|██████████| 4786/4786 [13:22<00:00,  5.96it/s]\n",
      "loss: 3.6: 100%|██████████| 4786/4786 [13:07<00:00,  6.07it/s]\n",
      "loss: 3.6: 100%|██████████| 4786/4786 [12:59<00:00,  6.14it/s]\n",
      "loss: 3.4: 100%|██████████| 4786/4786 [13:21<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n",
      "EPOCH 1 started==============================\n",
      "EPOCH 2 started==============================\n",
      "EPOCH 3 started==============================\n",
      "EPOCH 4 started==============================\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
    "                                            num_training_steps=EPOCHS*len(dataset))\n",
    "\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "\n",
    "    t = tqdm(enumerate(synopsis_loader), total=len(dataset), desc=\"loss: \")\n",
    "    for idx, batch in t:\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(batch, labels=batch)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        del outputs, loss, batch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if idx % 40 == 0:\n",
    "            t.set_description(f\"loss: {sum_loss/40 :.2}\")\n",
    "            sum_loss = 0\n",
    "\n",
    "    torch.save(model.state_dict(), str(PATH_TO_MODELS / f\"gpt2_epoch_{epoch}.pt\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Олег родился в 1970 году в Лос-Анджелесе в результате взрыва бомбы, превратившей весь город в руины, погибает 21 человек, а также его жена и двое детей от рук бандитов. Погибло не менее 20 человек и много других людей. В живых осталась только одна девушка. Она решает найти убийцу и убить его, но ее путь лежит через горы трупов и в горы мертвых городов, в которых нет ни души, кроме тех, кого она любит. На помощь ей приходят двое друзей: молодой врач и полицейский детектив. Они решают выследить убийцу, но вскоре понимают, что их миссия не такая простая как они могли бы подумать. На помощь приходит их бывший друг, который в совершенстве владеет искусством перевоплощения. Он решает помочь девушке спастись. Но его миссия должна будет стать еще более сложной, чем предыдущая, когда она расскажет ему о своих злоключениях, чтобы узнать правду о себе самой. Сможет ли она спасти свою семью и вернуть себе утраченные воспоминания и воспоминания о своих прошлых жизнях? Сможет ли она найти в себе мужество и не совершить непоправимого? И в этот раз она сама должна найти ответ на этот вопрос и помочь своим близким. Все они жаждут только одного: спасти свою семью и найти ответ на вопрос «Кто Я?». И вот перед ними возникает очень простой и очень сложный вопрос. Кто я? Как выбраться из города? Кто\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\" Олег родился в 1970 году \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}